---
title: "Proteomics univariate and multivariate combination"
author: "Clara Meijs"
date: "2023-07-21"
output:
  html_document:
    df_print: paged
    keep_md: yes
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 5
    theme: lumen
---

## Preparatory activities

Start with clearing environment and loading packages

```{r libraries, results='hide', message=FALSE,class.source = 'fold-hide'}
rm(list=ls())

library('ggplot2')
library('tidyverse')
library('Rtsne')
library('umap')
library('pheatmap')
library('RColorBrewer')
library("factoextra")
library("viridis")
library("dplyr")
library('caret')
library('ranger')
library('glmnet')
library('pROC')
library('RobustRankAggreg')
library('matrixStats') # row standard deviation
library('fgsea')
library('org.Hs.eg.db')
library('uwot')
library('h2o')
library('kernlab')
library('scales')
library('naniar')
library('plyr')
library('mice')
library('impute')
library('readxl')
library(limma)
library(IceR)



# define colours for plots
farben = viridis(2, option="C", direction = -1, begin = 0.2, end = 0.8)
```


```{r set-working-directories,message=FALSE,class.source = 'fold-hide'}
# if you are using Rstudio run the following command, otherwise, set the working directory to the folder where this script is in
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# create directory for results
dir.create(file.path(getwd(),'results'), showWarnings = FALSE)
# create directory for plots
dir.create(file.path(getwd(),'plots'), showWarnings = FALSE)
```

## Load data 

```{r load data}
df_ml = read.csv(file = 'data/df_ml.csv', header = T, row.names = 1)
DE =  read.csv(file = 'data/DE_analysis.csv', header = T, row.names = 1) 
```


## In-house developed functions

Jenny developed several functions to perform all upcoming analyses. In entails following functions: 

- runML()

- calculateROC()

- plotWeights()

- gsea()

- auccurve()

```{r functions, results='hide',class.source = 'fold-hide'}
###       boot strap function       ###
# runs machine learning on data with selected algorithm ( 'lm' (linear regression) or
#                                                         'rf' (random forest) or
#                                                         'svm lin' Support Vector Machine + linear kernel)
#                                                         'svm rad' Support Vector Machine + radial kernel))
# number of folds for cross validation (default = 10) and boot strap loops (default = 1)
#
# returns list objects with models, importance, predictions (raw and probability) and indices (samples used for training)


runML = function(data_frame,algorithm, cv = 10, BS_number = 1){
  #check input
  stopifnot('Last argument (BS_number) is not a number.' = is.numeric(BS_number),
            'Number given for cross validation is not a number ,' = is.numeric(cv),
            'Data is not a dataframe.' = is.data.frame(data_frame),
            'status column of df not correct' = all(df_ml$status == 1 | df_ml$status == 0),
            'unknown algorithm. please select \'lm\',\'rf\',\'svm rad\' or \'svm lin\' ' = algorithm %in% c('lm','rf','svm rad','svm lin'))
  
  key_output = c('linear regressio (lasso)','random forest', 'Support Vector Machine (radial kernel)', 'Support Vector Machine (linear kernel)')
  names(key_output) = c('lm','rf','svm rad','svm lin')
  # Output parameters for user check
  print(paste0('Running a ',BS_number,'x boot strap with a ',cv,' fold cross validation. Algorithm is ', key_output[[algorithm]]))
  
  data_frame$status = as.factor(make.names(data_frame$status)) # caret needs prediction variable to have name; turns 0 -> X0 and 1 -> X1
  
  # create lists to save results from bs
  models = list()
  importance = list()
  predictions = list()
  indices = list()
  predictions_raw = list()
  smp_size = floor(0.8 * nrow(data_frame)) # use 80% of data for training, 20% for testing
  
  #set the right parameters for each algo
  if(algorithm == 'lm'| algorithm == 'svm rad' | algorithm == 'svm lin'){
    ctrl = trainControl(method="cv",   
                        number = cv,        
                        summaryFunction=twoClassSummary,   # Use AUC to pick the best model
                        classProbs=TRUE,savePredictions = TRUE)
  }
  else if(algorithm == 'rf'){
    
    n_features = ncol(data_frame[ ,!names(data_frame) == 'status'])
    ctrl = trainControl(method = "cv", 
                        number = cv, 
                        search = 'grid',classProbs = TRUE, savePredictions = TRUE, summaryFunction=twoClassSummary )
  }
  
  # run machine learning
  for(i in 1:BS_number){
  
    train_ind = sample(seq_len(nrow(data_frame)), size = smp_size)
    train = data_frame[train_ind, ]
    test = data_frame[ -train_ind,!names(data_frame) == "status"]
    
    if (algorithm == 'lm'){
      model = train(x=train[ , !names(train) == "status"],
                    y= train$status,
                    method = "glmnet", family = "binomial", tuneLength = 5, metric = "ROC",
                    trControl = ctrl,
                    tuneGrid=expand.grid(
                      .alpha=1, # alpha 1 == lasso
                      .lambda=seq(0, 100, by = 0.1))
      )
    }
    
    else if (algorithm == 'rf'){
      tunegrid = expand.grid(
        .mtry = c(2, 3, 4, 7, 11, 17, 27, floor(sqrt(n_features)), 41, 64, 99, 154, 237, 367, 567, 876),
        .splitrule = c("extratrees","gini"),
        .min.node.size = c(1,2,3,4,5)
      )
      
      model = train(x=train[ , !names(train) == "status"],
                    y= train$status,
                    tuneGrid = tunegrid, 
                    method = "ranger",  tuneLength = 15, metric = "ROC",
                    num.trees = 500,
                    trControl = ctrl,
                    importance = 'impurity'
      )
    }
    
    else if (algorithm == 'svm lin'){
      model = train(x=train[ , !names(train) == "status"],
                    y= train$status,
                    method = "svmLinear", tuneLength = 5, metric = "ROC",
                    trControl = ctrl,
      )
    }
    
    else if (algorithm == 'svm rad'){
      model = train(x=train[ , !names(train) == "status"],
                    y= train$status,
                    method = "svmRadial", tuneLength = 5, metric = "ROC",
                    trControl = ctrl,
      )
    }
    
    models[[i]] = model
    importance[[i]] = varImp(model)
    predictions[[i]] = predict(model, newdata = test,type = "prob")
    predictions_raw[[i]] = predict(model, newdata = test,type = "raw")
    indices[[i]] = train_ind
    print(paste0('finshed loop #',i)) # keep track of what is happening
  }
  return_list = list(models,importance,predictions,predictions_raw,indices)
  names(return_list) = c('models','importance','predictions','predictions_raw','indices')
  return(return_list)
}

###       calculate averaged ROC curve        ###
# plots and returns data frame for averaged ROC curve from data and result from runML (indices, predictions (raw and probable))

calculateROC_jpeg = function(list_from_ML, data_frame, plot_path_pdf = FALSE, plot_path_jpeg){
  
  # extract needed lists from ML data
  indices = list_from_ML[['indices']]
  predictions = list_from_ML[['predictions']]
  predictions_raw = list_from_ML[['predictions_raw']]
  
  # create one data frame to compare predicted vs actual status
  predictions_compare = data.frame()
  for(i in 1:length(indices)){
    adding = data.frame(Class = data_frame$status[-indices[[i]]], predicted = predictions[[i]]$X0, resample = paste0("run ", i), auc = glmnet:::auc(data_frame$status[-indices[[i]]], predictions_raw[[i]]))
    predictions_compare = rbind(predictions_compare,adding)
  }
  
  # calculate ROC for each run
  rocc = data.frame()
  auc = c()
  for(run in unique(predictions_compare$resample)){
    onefold = dplyr::filter(predictions_compare, resample == run)
    auc = c(auc,onefold$auc[1])
    roc_run = roc(onefold$Class, onefold$predicted, direction = ">")
    rocc = rbind(rocc, data.frame(Sp = roc_run$specificities, Sn = roc_run$sensitivities, n = rep(1:length(roc_run$sensitivities))))
  }
  
  # aggregate the results and create new data frame
  Sp = aggregate(Sp ~ n, rocc, mean)$Sp
  Sn = aggregate(Sn ~ n, rocc, mean)$Sn
  errorSp = aggregate(Sp ~ n, rocc, sd)$Sp
  errorSn = aggregate(Sn ~ n, rocc, sd)$Sn
  plotci = data.frame(Sp,Sn,errorSp,errorSn)
  
  # plot if path is given
  if(plot_path_pdf != FALSE & plot_path_jpeg != FALSE){
    
    #pdf
    pdf(plot_path_pdf,paper="a4r", width = 11, height = 8)
    print(ggplot(plotci, aes(x=(1-Sp),y=Sn)) + geom_line(aes(color = "darkorange")) + theme_bw() +
      ggtitle("mean ROC curve and 95 % CI") +
      geom_ribbon(aes(ymin = (Sn - 0.95*errorSn), ymax = (Sn + 0.95*errorSn), xmin = (1-Sp - 0.95*errorSp), xmax = (1-Sp + 0.95*errorSp),
                      fill = "#B2B2B2"), alpha = 0.5) +
      #   scale_y_continuous(expand = c(0,0), limits = c(0,1.02)) + scale_x_continuous(expand = c(0,0), limits = c(-0.01,1)) +
      scale_color_manual(name = NULL, label = "mean", values = c("darkorange")) +
      scale_fill_manual(name = NULL, label = "95 % CI", values = c('#B2B2B2') ) +
      annotate("text", x = 0.2, y = 0.8, label = paste("mean AUC: ", round(mean(auc),2), "\u00B1 ", round(sd(auc),2)) ) +
      geom_abline(slope = 1, color="darkgrey", alpha = 0.3))
    dev.off()
    
    #jpeg
    jpeg(plot_path_jpeg, width = 900, height = 600)
    print(ggplot(plotci, aes(x=(1-Sp),y=Sn)) + geom_line(aes(color = "darkorange")) + theme_bw() +
      ggtitle("mean ROC curve and 95 % CI") +
      geom_ribbon(aes(ymin = (Sn - 0.95*errorSn), ymax = (Sn + 0.95*errorSn), xmin = (1-Sp - 0.95*errorSp), xmax = (1-Sp + 0.95*errorSp),
                      fill = "#B2B2B2"), alpha = 0.5) +
      #   scale_y_continuous(expand = c(0,0), limits = c(0,1.02)) + scale_x_continuous(expand = c(0,0), limits = c(-0.01,1)) +
      scale_color_manual(name = NULL, label = "mean", values = c("darkorange")) +
      scale_fill_manual(name = NULL, label = "95 % CI", values = c('#B2B2B2') ) +
      annotate("text", x = 0.2, y = 0.8, label = paste("mean AUC: ", round(mean(auc),2), "\u00B1 ", round(sd(auc),2)) ) +
      geom_abline(slope = 1, color="darkgrey", alpha = 0.3))
    dev.off()
  }
  
  return(list(plotci, round(mean(auc),2), round(sd(auc),2)))
  
}

###       Extract weights/importance and plot their average       ###
# plots top n (default = 50) proteins by averaged weight/importance
# returns plot (if path is given) and data frame

plotWeights_jpeg = function(list_from_ML, plot_path_pdf = FALSE, plot_path_jpeg = FALSE, number = 50){
  # extract models to continue based on which algorithm input is from
  models = list_from_ML[['models']]
  
  ## linear regression ##
  if(models[[1]]$method == 'glmnet'){ 
    importance = list_from_ML[['importance']]
    
    # extract weights from all models
    coefficient = list()
    for(i in 1:length(models)){
      coefficient[[i]] = coef.glmnet(models[[i]]$finalModel, models[[i]]$bestTune$lambda)
    }
    
    #sort the weights
    weights_lm = vector("list",0)
    for (i in 1:length(coefficient)) {
      for (j in 1:length(coefficient[[i]])) {
        if(j == 1 || coefficient[[i]][j,1] == 0){ # skip intercept
          next
        }
        else if (row.names(coefficient[[i]])[j] %in% names(weights_lm)) { # protein already has a list -> append vector with value
          weights_lm[[row.names(coefficient[[i]])[j]]] = c(weights_lm[[row.names(coefficient[[i]])[j]]],coefficient[[i]][j,1])
        }
        else if (! row.names(coefficient[[i]])[j] %in% names(weights_lm)) {
          weights_lm[[row.names(coefficient[[i]])[j]]] = c(coefficient[[i]][j])
        }
        else{
          print("Failed to extract weights from linear regression model")
          break
        }
      }
    }
    
    # calculate statistics and put into data frame
    avg = c()
    error = c()
    picks = c()
    for (i in 1:length(weights_lm)) {
      avg = c(avg,mean(weights_lm[[i]]))
      error = c(error, sd(weights_lm[[i]]))
      picks = c(picks,length(weights_lm[[i]]))
    }
    
    weight = data.frame(avg,error, picks,row.names=names(weights_lm))
    weight = weight[which(abs(weight$avg)>abs(weight$error) & weight$picks > 1), ]
    
    weight = weight[order(abs(weight$avg), decreasing = TRUE), ]
    name = 'weight'
  }
  
  ## random forest ##
  else if(models[[1]]$method == 'ranger'){ 
    rf_imp = list_from_ML[['importance']]
    
    # create data frame with names for all proteins
    weight = data.frame(row.names = row.names(rf_imp[[1]]$importance))
    
    # fill data frame
    for(i in 1:length(rf_imp)){
      weight = cbind(weight, rf_imp[[i]]$importance, by = "row.names")
      weight$by = NULL
      names(weight)[names(weight) == "Overall"] = paste("run",i, sep = "")
    }
    
    # calculate statistics into data frame
    weight$avg = rowMeans(weight)
    weight$error = rowSds(as.matrix(weight[,-(ncol(weight))]))
    
    weight = weight[order(weight$avg, decreasing = TRUE), ]
    name = 'importance'
  }
  
  ## svm linear kernel ##
  else if(models[[1]]$method == 'svmLinear'){ 
    # calculate weights for each run
    for(i in 1:length(models)){
      coef = models[[i]]$finalModel@coef[[1]]
      matr = models[[i]]$finalModel@xmatrix[[1]]
      
      weig = as.data.frame(coef %*% matr)
      
      if(i == 1){
        weight = weig
      }
      else if(i>1 & all(names(weig) == names(weight))){
        weight = rbind(weight,weig)
      }
      else{
        print(paste(i," ERROR: failed to extract protein weights from SVM linear; differing protein names between runs "))
      }
    }
    
    # calculate statistics and add to data frame
    weight= t(as.data.frame(weight))
    
    weight2 = as.data.frame(matrix(ncol = 2, nrow = nrow(weight), dimnames = 
                                     list(rownames(weight),c("avg","error"))))
    
    weight2$avg = rowMeans(weight)
    weight2$error = rowSds(weight)
    
    weight = weight2[order(abs(weight2$avg), decreasing = TRUE), ]
    name = 'weight'
  }
  
  # plot if path is given
  if(plot_path_pdf != FALSE & plot_path_jpeg != FALSE){
    
    #pdf 
    pdf(plot_path_pdf,paper="a4r", width = 11, height = 8)
    print(ggplot(weight[1:number, ], aes(x = reorder(rownames(weight[1:number, ]),avg), y = avg, fill = avg > 0)) +
            geom_bar(stat = "identity")+
            geom_errorbar( aes(x=reorder(rownames(weight[1:number, ]),avg), ymin=avg-error, ymax=avg+error, colour="#287D8EFF"), width=0.2, alpha=0.9, size=1.2) +
            xlab("Gene names") +
            ylab(name) +
            ggtitle(paste("Mean ",name," and standard deviation; ",models[[1]]$method)) +
            theme_classic() +
            theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
            scale_x_discrete(labels = abbreviate) + 
            scale_fill_manual(name = NULL, label = NULL, values = c("darkorange", "darkgrey")) +
            scale_color_manual(name = NULL, label = "standard deviation", values = c("darkblue")) +
            guides(fill = 'none', colour = 'none'))
    dev.off()
    
    #jpeg
    jpeg(plot_path_jpeg, width = 900, height = 600)
    print(ggplot(weight[1:number, ], aes(x = reorder(rownames(weight[1:number, ]),avg), y = avg, fill = avg > 0)) +
            geom_bar(stat = "identity")+
            geom_errorbar( aes(x=reorder(rownames(weight[1:number, ]),avg), ymin=avg-error, ymax=avg+error, colour="#287D8EFF"), width=0.2, alpha=0.9, size=1.2) +
            xlab("Gene names") +
            ylab(name) +
            ggtitle(paste("Mean ",name," and standard deviation; ",models[[1]]$method)) +
            theme_classic() +
            theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
            scale_x_discrete(labels = abbreviate) + 
            scale_fill_manual(name = NULL, label = NULL, values = c("darkorange", "darkgrey")) +
            scale_color_manual(name = NULL, label = "standard deviation", values = c("darkblue")) +
            guides(fill = 'none', colour = 'none'))
    dev.off()
  }
  return(weight)
}

###       Run Gene Set Enrichment analysis on given background and vector of genes       ###
# plots top n (default = 20) enriched KEGG pathways
# returns plot (if path is given) and data frame

gsea_jpeg = function(genes, min_size = 3,plot_path_pdf = FALSE, plot_path_jpeg = FALSE, number = 20, entrezID = FALSE, background){
  stopifnot('minSize has to be a number' = is.numeric(min_size),
            'background has to be a vector of type character' = is.vector(background, mode = 'character'),
            'genes has to be a named vector with numeric value' = is.vector(genes, mode = 'numeric') & !any(is.na(names(genes))) | missing(background) )
  
  ## load pathways
  kegg_pathways = gmtPathways("data/c2.cp.kegg.v7.5.1.entrez.gmt")
  
  ## get entrezID of background and filter pathways
  # set mapping direction
  keyt = ifelse(entrezID == FALSE, 'SYMBOL', 'ENTREZID')
  
  if(!missing(background)){
    ids_b = AnnotationDbi::select(org.Hs.eg.db, 
                   keys = background,
                   columns = c("ENTREZID", "SYMBOL"),
                   keytype = keyt)
    background_id = ids_b$ENTREZID
    # let user now if certain proteins were excluded due to not mapped EntrezID
    if(any(is.na(background_id))){
      background_id = background_id[!is.na(background_id)]
      print(paste0('Could not find EntrezID for ', ids_b$SYMBOL[which(is.na(ids_b$ENTREZID))]),'. These will be ignored in further analysis.')
    }
    
    # filter KEGG pathways
    for(i in c(1:length(kegg_pathways))){
      kegg_pathways[[i]] = kegg_pathways[[i]][which(kegg_pathways[[i]] %in% background_id)]
    }
  }
  
  ## get entrezID of genes
  ids_g = AnnotationDbi::select(org.Hs.eg.db, 
                 keys = names(genes),
                 columns = c("ENTREZID", "SYMBOL"),
                 keytype = keyt)
  
  gene_id = ids_g$ENTREZID
  names(genes) = gene_id
  
  # let user now if certain proteins were excluded due to not mapped EntrezID
  if(any(is.na(ids_g$ENTREZID))){
    genes = genes[!is.na(names(genes))]
    print(paste0('Could not find EntrezID for', ids_g$SYMBOL[which(is.na(ids_g$ENTREZID))],'. These will be ignored in further analysis.' ))
  }
  
  ## run fgsea()
  # set scoreType depending on data 
  scoret = ifelse(all(genes >= 0), 'pos','std')
  
  fgsea_results = fgsea(pathways = kegg_pathways,
                        stats    = genes,
                        minSize  = min_size,
                        maxSize  = 500,
                        scoreType = scoret)
  
  # calculate number of genes in pathways 
  fgsea_results$count = sapply(fgsea_results$leadingEdge,length)
  # with small gene sample count is sometimes 0 -> filter
  fgsea_results = fgsea_results[fgsea_results$count != 0, ]
  
  # store gene names, not just IDs in column
  if(!missing(background)){
    fgsea_results$geneNames =   sapply(fgsea_results$leadingEdge, function(x) paste0(ids_g$SYMBOL[which(ids_g$ENTREZID %in% x)], collapse = ', '))
  }
  
  # get the actual length of the pathways as column
  length_pathways = as.data.frame(lapply(kegg_pathways,length))
  length_pathways = as.data.frame(t(length_pathways))
  names(length_pathways) = 'pathwaySize'
  length_pathways$pathway = row.names(length_pathways)
  
  # merge into one df 
  fgsea_results = merge(fgsea_results,length_pathways, by = "pathway")
  
  # plot if path is given
  if(plot_path_pdf != FALSE & plot_path_jpeg != FALSE){
    
    #pdf
    pdf(plot_path_pdf,paper="a4r", width = 11, height = 8)
    print(ggplot(fgsea_results[order(fgsea_results$padj), ][1:number, ], aes(x = -log10(padj), y = reorder(pathway, -padj), fill = padj)) +
      geom_bar(stat = "identity") +
      xlab("- log(p-value)") +
      ylab("Functional category") +
      ggtitle("KEGG terms") +
      labs(fill = "FDR value") +
      scale_fill_viridis(option = "E")+ 
      geom_text(aes(label = paste0(count, "/", pathwaySize)), hjust=-0.15, size =3.5)+ 
      xlim(0, max(-log10(fgsea_results$padj))+0.02))
    dev.off()
    
    #jpeg
    jpeg(plot_path_jpeg,width = 900, height = 600)
    print(ggplot(fgsea_results[order(fgsea_results$padj), ][1:number, ], aes(x = -log10(padj), y = reorder(pathway, -padj), fill = padj)) +
      geom_bar(stat = "identity") +
      xlab("- log(p-value)") +
      ylab("Functional category") +
      ggtitle("KEGG terms") +
      labs(fill = "FDR value") +
      scale_fill_viridis(option = "E")+ 
      geom_text(aes(label = paste0(count, "/", pathwaySize)), hjust=-0.15, size =3.5)+ 
      xlim(0, max(-log10(fgsea_results$padj))+0.02))
    dev.off()
    
  }
  
  return(fgsea_results)
}

### AUC curve of linear model from previously calculated weights
# calculates AUC of a linear model on a data set (protein_data) based on a given protein combination with average weights from previous linear model boot strapping
# returns data frame for plotting combinations to auc
auccurve = function(vectornames,weight_data,protein_data, maxn, add = F){
  stopifnot(
    'gene names in vector not represented in data frame' = vectornames %in% row.names(weight_data),
    'vectornames is not a character vector' = is.vector(vectornames, mode = 'character')
  )
  
  # if no maxn is given all possible combinations will be built
  if(missing(maxn)){
      maxn = length(vectornames)
    }
    
    # prepare combinations of defined length
    if(add == F){
    res = Map(combn, list(vectornames), seq_along(c(1:maxn)), simplyfy = F)
    test = unlist(res, recursive = FALSE)
    vectors = list()
    z=1
    for(i in 1:(length(res))){
      for(j in 1:(length(res[[i]])/length(res[[i]][ ,1]) ) ){
        vectors[[z]] = res[[i]][,j]
        z = z + 1
      }
    }
    }
  # if add is set to TRUE, combinations are only adding one protein after the other in order they are given (going from legnth 1 to length of vectornames)
  else{
    vectors = list()
    z=1
    for(i in 1:(length(vectornames))){
      
      vectors[[i]] = vectornames[1:i]
      z = z + 1
    }
  }
  
  ## put combinations into empty dataframe
  savespace = data.frame(combinations = I(vectors))
  savespace$auc = NA
  
  ## add auc for the combinations
  for(i in 1:nrow(savespace)){
    interim = as.data.frame(weight_data)
    # set all weights that are not in the combination of interes to 0
    interim[! row.names(interim) %in% vectors[[i]], "V1"] = 0
    # predict 0 or 1 by using the linear regression formula (y = w1*x1+ .... w_n*x_n)
    interim_pred = as.matrix(as.matrix(protein_data[ ,!names(protein_data)=='status']) %*% as.matrix(interim))
    # use glmnet function to calculate the auc between actual status and the calculated status, i.e. prediction
    savespace$auc[i] = glmnet:::auc(interim_pred,protein_data$status)
  }

  savespace$combinations = lapply(savespace$combinations, function(X) paste0(X,collapse = ", "))
  return(savespace)
}

spec_and_sens= function(data_frame){
      #create vectors for values
      spec = sens = rep(NA,length(data_frame[["models"]]))
      
      #pull out model performance values
      for(i in 1:length(data_frame[["models"]])){
        spec[i] = as.numeric(data_frame[["models"]][[i]][["results"]]["Spec"])
        sens[i] = as.numeric(data_frame[["models"]][[i]][["results"]]["Sens"])
      }
      
      #return means of specificty and sensitivity
      return(cat("mean specificity is ", round(mean(spec),2), " and mean sensitivity is ", round(mean(sens),2)))
}

```


```{r set parameters for loops}
machines = c("svm lin", "svm rad", "lm", "rf")
machines_text = c("svm_l", "svm_r", "lm", "rf")
DE_cutoff = c(1:10)
DE_cutoff_text = c("all", "top_half", "top_third", "top_fourth", "top_fifth", "top_sixth", "top_seventh", "top_eigth", "top_nineth", "top_tenth")
bs = 500
```

```{r ML loops with DE before bootstrap}

for(i in 1:length(machines)){
  for(j in 1:length(DE_cutoff)){

    q = c("status", rownames(DE)[1:(nrow(DE)/DE_cutoff[j])])
    title = paste0(machines_text[i], "_", DE_cutoff_text[j])
    print(title)
    data = df_ml[,colnames(df_ml) %in% q]

    if(!file.exists(paste0('results/roc_', title, '.csv'))){
          
    #run model
    m = runML(data, machines[i], BS_number = bs)
      
    # save results for later use
    saveRDS(m, file = paste0('results/', title, '.rds'))
    
    #plot ROC curve
    ROC_curve_l = calculateROC_jpeg(m, data, 
                                  paste0('plots/roc_', title, '.pdf'),
                                  paste0('plots/roc_', title, '.jpeg'))
    ROC_curve = ROC_curve_l[1]
    auc_m = ROC_curve_l[2]
    auc_sd = ROC_curve_l[3]
    auc = cbind(auc_m, auc_sd)

    # save results for later use
    write.csv(ROC_curve, file = paste0('results/roc_', title, '.csv'))
    write.csv(auc, file = paste0('results/auc_', title, '.csv'))

    if(machines[i]!="svm rad"){
      # extract weights + plot averaged
    m_weights = plotWeights_jpeg(m, 
                                  paste0('plots/weights_', title, '.pdf'),
                                  paste0('plots/weights_', title, '.jpeg'))
    # save results for later use
    write.csv(m_weights, file = paste0('results/weights_', title, '.csv'))
    
    
  }}
}}

```


```{r calculate AUCs for all methods}
percentile = c()
for(i in 1:max(DE_cutoff)){
  percentile[i] = 1/i
}

for(i in 1:length(machines)){
  a_v = sd_v = c()
  for(j in 1:length(DE_cutoff)){
    title = paste0(machines_text[i], "_", DE_cutoff_text[j])
    if(file.exists(paste0('results/auc_', title, '.csv'))){
      
    d = read.csv(file = paste0('results/auc_', title, '.csv'))

    a_v[j] = d$auc_m
    sd_v[j] = d$auc_sd
    }
    
    }
  if(i == 1){
    auc = as.data.frame(cbind(a_v, sd_v, percentile[1:length(a_v)], rep(machines[i],length(a_v))))
  }
  if(i>1){
    auc = rbind(auc, as.data.frame(cbind(a_v, sd_v, percentile[1:length(a_v)], rep(machines[i],length(a_v)))))
  }
}

colnames(auc) = c("auc", "sd", "percentile", "machine")
auc[,1:3] = apply(auc[,1:3], MARGIN = 2, FUN = as.numeric)
auc$low = auc$auc - auc$sd/2
auc$high = auc$auc + auc$sd/2

library(ggthemes)

ggplot(data = auc, aes(x=percentile, y=auc, group=machine)) +
  geom_line(aes(col = machine)) +
  geom_point(aes(col = machine)) +
  labs(y= "AUC", x = "percentile") +
  ylim(0.5, 1) +
  geom_ribbon(aes(ymin = low, ymax = high), alpha = 0.1) +
  theme_few() +
  scale_colour_few()

ggsave("plots/AUC_DE_cutoffs_all_machines.pdf", width = 11/2, height = 8/2, units = "in")
save(auc, file = 'results/auc_DE_cutoffs.rds')


```

```{r scatter plots comparing weigths of proteins}
#load model from svm lin all variables
i = 1; j = 1
title = paste0(machines_text[i], "_", DE_cutoff_text[j])
w1 = read.csv(file = paste0('results/weights_', title, '.csv'), row.names = 1)

#load model from svm lin 1/3 variables
i = 1; j = 3
title = paste0(machines_text[i], "_", DE_cutoff_text[j])
w2 = read.csv(file = paste0('results/weights_', title, '.csv'), row.names = 1)

#list of variables only in w1
only_w1 = w1[!rownames(w1) %in% rownames(w2),]

#select only variables present in both models
w1 = w1[rownames(w1) %in% rownames(w2),]

#reorder w2
w2 = w2[rownames(w1),]

weights = as.data.frame(cbind(w1, w2))
colnames(weights) = c("avg_w1", "error_w1", "avg_w2", "error_w2")

#scatterplot weights 
ggplot(weights, aes(x=avg_w1, y=avg_w2)) + 
  geom_point() +
  labs(y= "variable weights smaller model", x = "variable weights bigger model") +
  theme_few() +
  scale_colour_few()

ggsave("plots/scatterplot_weights_svm_lin_all_and_one_third.pdf", width = 11/2, height = 8/2, units = "in")
```

```{r box plot weights}
#recombine the variables with labels where they come from
w_both = cbind(w1, rep("both", nrow(w1)))
w_only_w1 = cbind(only_w1, rep("only_w1", nrow(only_w1)))
colnames(w_both)[3] = colnames(w_only_w1)[3] = "model_inclusion"

weights_full_model = as.data.frame(rbind(w_both, w_only_w1))
weights_full_model$avg = abs(weights_full_model$avg)

ggplot(weights_full_model, aes(x=avg, y=model_inclusion)) + 
  geom_boxplot() + coord_flip() + theme_few() +
  labs(x= "absolute weights", y = "in which models the variables were included")

ggsave("plots/boxplot_weights_svm_lin_all_and_one_third.pdf", width = 5, height = 5, units = "in")
```

```{r ranking proteins previous project}
p = c("CRYM", "CAPZA2", "ALDH16A1", "PFKL", "SERPINC1", "HP", "EIF2S2", "GMPPA", "SCGB1D1")
w2$rank = 1:nrow(w2)
w2$percentile_rank = round((1:nrow(w2))/nrow(w2)*100,2)
w2[p,]
```

```{r adjust ML function to include the DE analysis in the bootstrap}
###       boot strap function       ###
# runs machine learning on data with selected algorithm ( 'lm' (linear regression) or
#                                                         'rf' (random forest) or
#                                                         'svm lin' Support Vector Machine + linear kernel)
#                                                         'svm rad' Support Vector Machine + radial kernel))
# number of folds for cross validation (default = 10) and boot strap loops (default = 1)
#
# returns list objects with models, importance, predictions (raw and probability) and indices (samples used for training)


runML_incl_DE = function(data_frame,algorithm, cv = 10, BS_number = 1, DE_sample_size = 3, DE_percentile = 1){
  #check input
  stopifnot('Last argument (BS_number) is not a number.' = is.numeric(BS_number),
            'Number given for cross validation is not a number ,' = is.numeric(cv),
            'Data is not a dataframe.' = is.data.frame(data_frame),
            'status column of df not correct' = all(df_ml$status == 1 | df_ml$status == 0),
            'unknown algorithm. please select \'lm\',\'rf\',\'svm rad\' or \'svm lin\' ' = algorithm %in% c('lm','rf','svm rad','svm lin'))
  
  key_output = c('linear regressio (lasso)','random forest', 'Support Vector Machine (radial kernel)', 'Support Vector Machine (linear kernel)')
  names(key_output) = c('lm','rf','svm rad','svm lin')
  # Output parameters for user check
  print(paste0('Running a ',BS_number,'x boot strap with a ',cv,' fold cross validation. Algorithm is ', key_output[[algorithm]]))


  # create lists to save results from bs
  models = list()
  importance = list()
  predictions = list()
  indices = list()
  predictions_raw = list()
  data_frames = list()
  d = data_frame

  
  
  #set the right parameters for each algo
  if(algorithm == 'lm'| algorithm == 'svm rad' | algorithm == 'svm lin'){
    ctrl = trainControl(method="cv",   
                        number = cv,        
                        summaryFunction=twoClassSummary,   # Use AUC to pick the best model
                        classProbs=TRUE,savePredictions = TRUE)
  }
  else if(algorithm == 'rf'){
    
    n_features = round(ncol(data_frame[ ,!names(data_frame) == 'status'])*DE_percentile,0)
    ctrl = trainControl(method = "cv", 
                        number = cv, 
                        search = 'grid',classProbs = TRUE, savePredictions = TRUE, summaryFunction=twoClassSummary )
  }
  
  # run machine learning
  for(i in 1:BS_number){

    ############ THE DEx PART #################################
  
  #take subset of dataset according to DE
  DE_ind_case = sample(seq_len(nrow(d[d$status==1,])), size = DE_sample_size)
  DE_ind_control = sample(seq_len(nrow(d[d$status==0,])), size = DE_sample_size) + sum(d$status==1)
  DE_ind = c(DE_ind_case, DE_ind_control)
  
  d_DE = d[DE_ind,]
  DE = LIMMA_analysis(
                data = as.data.frame(t(d[,2:ncol(d_DE)])),
                assignments = d_DE[,1])
  
  selected_variables = c("status", rownames(DE)[1:round(nrow(DE)*DE_percentile,0)])
  data_frame = d[-DE_ind, selected_variables]
    
    ####### END DEx PART #################################        
    
      
    smp_size = floor(0.8 * nrow(data_frame)) # use 80% of data for training, 20% for testing
    data_frame$status = as.factor(make.names(data_frame$status)) # caret needs prediction variable to have name;      turns 0 -> X0 and 1 -> X1
    train_ind = sample(seq_len(nrow(data_frame)), size = smp_size)
    train = data_frame[train_ind, ]
    test = data_frame[ -train_ind,!names(data_frame) == "status"]
    

    
    if (algorithm == 'lm'){
      model = train(x=train[ , !names(train) == "status"],
                    y= train$status,
                    method = "glmnet", family = "binomial", tuneLength = 5, metric = "ROC",
                    trControl = ctrl,
                    tuneGrid=expand.grid(
                      .alpha=1, # alpha 1 == lasso
                      .lambda=seq(0, 100, by = 0.1))
      )
    }
    
    else if (algorithm == 'rf'){
      tunegrid = expand.grid(
        .mtry = c(2, 3, 4, 7, 11, 17, 27, floor(sqrt(n_features)), 41, 64, 99, 154, 237, 367, 567, 876),
        .splitrule = c("extratrees","gini"),
        .min.node.size = c(1,2,3,4,5)
      )
      
      model = train(x=train[ , !names(train) == "status"],
                    y= train$status,
                    tuneGrid = tunegrid, 
                    method = "ranger",  tuneLength = 15, metric = "ROC",
                    num.trees = 500,
                    trControl = ctrl,
                    importance = 'impurity'
      )
    }
    
    else if (algorithm == 'svm lin'){
      model = train(x=train[ , !names(train) == "status"],
                    y= train$status,
                    method = "svmLinear", tuneLength = 5, metric = "ROC",
                    trControl = ctrl,
      )
    }
    
    else if (algorithm == 'svm rad'){
      model = train(x=train[ , !names(train) == "status"],
                    y= train$status,
                    method = "svmRadial", tuneLength = 5, metric = "ROC",
                    trControl = ctrl,
      )
    }
    
    models[[i]] = model
    importance[[i]] = varImp(model)
    predictions[[i]] = predict(model, newdata = test, type = "prob")
    predictions_raw[[i]] = predict(model, newdata = test,type = "raw")
    indices[[i]] = train_ind
    data_frames[[i]] = data_frame
    print(paste0('finshed loop #',i)) # keep track of what is happening
  }
  return_list = list(models,importance,predictions,predictions_raw,indices, selected_variables, data_frames)
  names(return_list) = c('models','importance','predictions','predictions_raw','indices', 'selected_variables', 'data_frames')
  return(return_list)
}


###       calculate averaged ROC curve        ###
# plots and returns data frame for averaged ROC curve from data and result from runML (indices, predictions (raw and probable))

calculateROC_jpeg = function(list_from_ML, plot_path_pdf = FALSE, plot_path_jpeg){
  
  # extract needed lists from ML data
  indices = list_from_ML[['indices']]
  predictions = list_from_ML[['predictions']]
  predictions_raw = list_from_ML[['predictions_raw']]
  data_frames = list_from_ML[['data_frames']]
  
  # create one data frame to compare predicted vs actual status
  predictions_compare = data.frame()
  for(i in 1:length(indices)){
    adding = data.frame(Class = data_frames[[i]]$status[-indices[[i]]], predicted = predictions[[i]]$X0, resample = paste0("run ", i), auc = glmnet:::auc(data_frames[[i]]$status[-indices[[i]]], predictions_raw[[i]]))
    predictions_compare = rbind(predictions_compare,adding)
  }
  
  # calculate ROC for each run
  rocc = data.frame()
  auc = c()
  for(run in unique(predictions_compare$resample)){
    onefold = dplyr::filter(predictions_compare, resample == run)
    auc = c(auc,onefold$auc[1])
    roc_run = roc(onefold$Class, onefold$predicted, direction = ">")
    rocc = rbind(rocc, data.frame(Sp = roc_run$specificities, Sn = roc_run$sensitivities, n = rep(1:length(roc_run$sensitivities))))
  }
  
  # aggregate the results and create new data frame
  Sp = aggregate(Sp ~ n, rocc, mean)$Sp
  Sn = aggregate(Sn ~ n, rocc, mean)$Sn
  errorSp = aggregate(Sp ~ n, rocc, sd)$Sp
  errorSn = aggregate(Sn ~ n, rocc, sd)$Sn
  plotci = data.frame(Sp,Sn,errorSp,errorSn)
  
  # plot if path is given
  if(plot_path_pdf != FALSE & plot_path_jpeg != FALSE){
    
    #pdf
    pdf(plot_path_pdf,paper="a4r", width = 11, height = 8)
    print(ggplot(plotci, aes(x=(1-Sp),y=Sn)) + geom_line(aes(color = "darkorange")) + theme_bw() +
      ggtitle("mean ROC curve and 95 % CI") +
      geom_ribbon(aes(ymin = (Sn - 0.95*errorSn), ymax = (Sn + 0.95*errorSn), xmin = (1-Sp - 0.95*errorSp), xmax = (1-Sp + 0.95*errorSp),
                      fill = "#B2B2B2"), alpha = 0.5) +
      #   scale_y_continuous(expand = c(0,0), limits = c(0,1.02)) + scale_x_continuous(expand = c(0,0), limits = c(-0.01,1)) +
      scale_color_manual(name = NULL, label = "mean", values = c("darkorange")) +
      scale_fill_manual(name = NULL, label = "95 % CI", values = c('#B2B2B2') ) +
      annotate("text", x = 0.2, y = 0.8, label = paste("mean AUC: ", round(mean(auc),2), "\u00B1 ", round(sd(auc),2)) ) +
      geom_abline(slope = 1, color="darkgrey", alpha = 0.3))
    dev.off()
    
    #jpeg
    jpeg(plot_path_jpeg, width = 900, height = 600)
    print(ggplot(plotci, aes(x=(1-Sp),y=Sn)) + geom_line(aes(color = "darkorange")) + theme_bw() +
      ggtitle("mean ROC curve and 95 % CI") +
      geom_ribbon(aes(ymin = (Sn - 0.95*errorSn), ymax = (Sn + 0.95*errorSn), xmin = (1-Sp - 0.95*errorSp), xmax = (1-Sp + 0.95*errorSp),
                      fill = "#B2B2B2"), alpha = 0.5) +
      #   scale_y_continuous(expand = c(0,0), limits = c(0,1.02)) + scale_x_continuous(expand = c(0,0), limits = c(-0.01,1)) +
      scale_color_manual(name = NULL, label = "mean", values = c("darkorange")) +
      scale_fill_manual(name = NULL, label = "95 % CI", values = c('#B2B2B2') ) +
      annotate("text", x = 0.2, y = 0.8, label = paste("mean AUC: ", round(mean(auc),2), "\u00B1 ", round(sd(auc),2)) ) +
      geom_abline(slope = 1, color="darkgrey", alpha = 0.3))
    dev.off()
  }
  
  return(list(plotci, round(mean(auc),2), round(sd(auc),2)))
  
}

plotWeights_jpeg = function(list_from_ML, plot_path_pdf = FALSE, plot_path_jpeg = FALSE, number = 50){
  # extract models to continue based on which algorithm input is from
  models = list_from_ML[['models']]
  
  ## linear regression ##
  if(models[[1]]$method == 'glmnet'){ 
    importance = list_from_ML[['importance']]
    
    # extract weights from all models
    coefficient = list()
    for(i in 1:length(models)){
      coefficient[[i]] = coef.glmnet(models[[i]]$finalModel, models[[i]]$bestTune$lambda)
    }
    
    #sort the weights
    weights_lm = vector("list",0)
    for (i in 1:length(coefficient)) {
      for (j in 1:length(coefficient[[i]])) {
        if(j == 1 || coefficient[[i]][j,1] == 0){ # skip intercept
          next
        }
        else if (row.names(coefficient[[i]])[j] %in% names(weights_lm)) { # protein already has a list -> append vector with value
          weights_lm[[row.names(coefficient[[i]])[j]]] = c(weights_lm[[row.names(coefficient[[i]])[j]]],coefficient[[i]][j,1])
        }
        else if (! row.names(coefficient[[i]])[j] %in% names(weights_lm)) {
          weights_lm[[row.names(coefficient[[i]])[j]]] = c(coefficient[[i]][j])
        }
        else{
          print("Failed to extract weights from linear regression model")
          break
        }
      }
    }
    
    # calculate statistics and put into data frame
    avg = c()
    error = c()
    picks = c()
    for (i in 1:length(weights_lm)) {
      avg = c(avg,mean(weights_lm[[i]]))
      error = c(error, sd(weights_lm[[i]]))
      picks = c(picks,length(weights_lm[[i]]))
    }
    
    weight = data.frame(avg,error, picks,row.names=names(weights_lm))
    weight = weight[which(abs(weight$avg)>abs(weight$error) & weight$picks > 1), ]
    
    weight = weight[order(abs(weight$avg), decreasing = TRUE), ]
    name = 'weight'
  }
  
  ## random forest ##
  else if(models[[1]]$method == 'ranger'){ 
    rf_imp = list_from_ML[['importance']]
    
    # create data frame with names for all proteins
    weight = data.frame(row.names = row.names(rf_imp[[1]]$importance))
    
    # fill data frame
    for(i in 1:length(rf_imp)){
      weight = cbind(weight, rf_imp[[i]]$importance, by = "row.names")
      weight$by = NULL
      names(weight)[names(weight) == "Overall"] = paste("run",i, sep = "")
    }
    
    # calculate statistics into data frame
    weight$avg = rowMeans(weight)
    weight$error = rowSds(as.matrix(weight[,-(ncol(weight))]))
    
    weight = weight[order(weight$avg, decreasing = TRUE), ]
    name = 'importance'
  }
  
  ## svm linear kernel ##
  else if(models[[1]]$method == 'svmLinear'){
    
    #get a vector of all proteins that were included at least once
    proteins = c()
    for(i in 1:length(models)){
      matr = models[[i]]$finalModel@xmatrix[[1]]
      proteins = c(proteins, colnames(matr))
    }
    proteins = unique(proteins)
    weight = matrix(NA, nrow = length(models), ncol = length(proteins))
    colnames(weight) = proteins
    
    # calculate weights for each run
    for(i in 1:length(models)){
      coef = models[[i]]$finalModel@coef[[1]]
      matr = models[[i]]$finalModel@xmatrix[[1]]
      weig = as.data.frame(coef %*% matr)
      
      for(j in 1:length(proteins)){
        if(proteins[j] %in% colnames(weig))
          {
            weight[i,j] = weig[,proteins[j]]
          }
      }

    }
    
    # calculate statistics and add to data frame
    weight= t(as.data.frame(weight))
    
    weight2 = as.data.frame(matrix(ncol = 2, nrow = nrow(weight), dimnames = 
                                     list(rownames(weight),c("avg","error"))))
    
    weight2$avg = rowMeans(weight)
    weight2$error = rowSds(weight)
    
    weight = weight2[order(abs(weight2$avg), decreasing = TRUE), ]
    name = 'weight'
  }
  
  # plot if path is given
  if(plot_path_pdf != FALSE & plot_path_jpeg != FALSE){
    
    #pdf 
    pdf(plot_path_pdf,paper="a4r", width = 11, height = 8)
    print(ggplot(weight[1:number, ], aes(x = reorder(rownames(weight[1:number, ]),avg), y = avg, fill = avg > 0)) +
            geom_bar(stat = "identity")+
            geom_errorbar( aes(x=reorder(rownames(weight[1:number, ]),avg), ymin=avg-error, ymax=avg+error, colour="#287D8EFF"), width=0.2, alpha=0.9, size=1.2) +
            xlab("Gene names") +
            ylab(name) +
            ggtitle(paste("Mean ",name," and standard deviation; ",models[[1]]$method)) +
            theme_classic() +
            theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
            scale_x_discrete(labels = abbreviate) + 
            scale_fill_manual(name = NULL, label = NULL, values = c("darkorange", "darkgrey")) +
            scale_color_manual(name = NULL, label = "standard deviation", values = c("darkblue")) +
            guides(fill = 'none', colour = 'none'))
    dev.off()
    
    #jpeg
    jpeg(plot_path_jpeg, width = 900, height = 600)
    print(ggplot(weight[1:number, ], aes(x = reorder(rownames(weight[1:number, ]),avg), y = avg, fill = avg > 0)) +
            geom_bar(stat = "identity")+
            geom_errorbar( aes(x=reorder(rownames(weight[1:number, ]),avg), ymin=avg-error, ymax=avg+error, colour="#287D8EFF"), width=0.2, alpha=0.9, size=1.2) +
            xlab("Gene names") +
            ylab(name) +
            ggtitle(paste("Mean ",name," and standard deviation; ",models[[1]]$method)) +
            theme_classic() +
            theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
            scale_x_discrete(labels = abbreviate) + 
            scale_fill_manual(name = NULL, label = NULL, values = c("darkorange", "darkgrey")) +
            scale_color_manual(name = NULL, label = "standard deviation", values = c("darkblue")) +
            guides(fill = 'none', colour = 'none'))
    dev.off()
  }
  return(weight)
}


```

```{r machine learning loops including DE in bootstrap}

DE_cutoff = c(1, 0.75, 0.5, 0.33, 0.25, 0.1)
bs = 500

for(i in 1:length(machines)){
  for(j in 1:length(DE_cutoff)){
    title = paste0(machines_text[i], "_", DE_cutoff[j], "_DE_in_bootstrap")
    print(title)
    data = df_ml

    if(!file.exists(paste0('results/roc_', title, '.csv'))){
          
    #run model
    m = runML_incl_DE(data, machines[i], BS_number = bs, DE_percentile = DE_cutoff[j])
      
    # save results for later use
    saveRDS(m, file = paste0('results/', title, '.rds'))
    
    #plot ROC curve
    ROC_curve_l = calculateROC_jpeg(m, #data, 
                                  paste0('plots/roc_', title, '.pdf'),
                                  paste0('plots/roc_', title, '.jpeg'))
    ROC_curve = ROC_curve_l[1]
    auc_m = ROC_curve_l[2]
    auc_sd = ROC_curve_l[3]
    auc = cbind(auc_m, auc_sd)

    # save results for later use
    write.csv(ROC_curve, file = paste0('results/roc_', title, '.csv'))
    write.csv(auc, file = paste0('results/auc_', title, '.csv'))

    if(machines[i]!="svm rad"){
      # extract weights + plot averaged
    m_weights = plotWeights_jpeg(m, 
                                  paste0('plots/weights_', title, '.pdf'),
                                  paste0('plots/weights_', title, '.jpeg'))
    # save results for later use
    write.csv(m_weights, file = paste0('results/weights_', title, '.csv'))
  }}
  }
}

```

```{r calculate AUCs for all methods with DE in bootstrap}
percentile = DE_cutoff

for(i in 1:length(machines)){
  a_v = sd_v = c()
  for(j in 1:length(DE_cutoff)){
    print(machines[i]); print(DE_cutoff[j])
    title = paste0(machines_text[i], "_", DE_cutoff[j], "_DE_in_bootstrap")
    if(file.exists(paste0('results/auc_', title, '.csv'))){
      
    d = read.csv(file = paste0('results/auc_', title, '.csv'))

    a_v[j] = d$auc_m
    sd_v[j] = d$auc_sd
    }
    
    }
  if(i == 1){
    auc = as.data.frame(cbind(a_v, sd_v, percentile[1:length(a_v)], rep(machines[i],length(a_v))))
  }
  if(i>1){
    auc = rbind(auc, as.data.frame(cbind(a_v, sd_v, percentile[1:length(a_v)], rep(machines[i],length(a_v)))))
  }
}

colnames(auc) = c("auc", "sd", "percentile", "machine")
auc[,1:3] = apply(auc[,1:3], MARGIN = 2, FUN = as.numeric)
auc$low = auc$auc - auc$sd/2
auc$high = auc$auc + auc$sd/2

library(ggthemes)

ggplot(data = auc, aes(x=percentile, y=auc, group=machine)) +
  geom_line(aes(col = machine)) +
  geom_point(aes(col = machine)) +
  labs(y= "AUC", x = "percentile") +
  ylim(0.5, 1) +
  geom_ribbon(aes(ymin = low, ymax = high), alpha = 0.1) +
  theme_few() +
  scale_colour_few()

ggsave("plots/AUC_DE_cutoffs_all_machines_DE_in_bootstrap.pdf", width = 11/2, height = 8/2, units = "in")
save(auc, file = 'results/auc_DE_cutoffs_DE_in_bootstrap.rds')


```


